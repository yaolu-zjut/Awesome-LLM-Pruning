# Awesome-LLM-Pruning

This repository is dedicated to the pruning of large language models (LLMs). It aims to serve as a comprehensive resource for researchers and practitioners interested in the efficient reduction of model size while maintaining or enhancing performance.

# Contents
- Papers
  - Survey
  - Layer Pruning
- Fine-tuning method
- Models
- Datasets 
- Tools


## Papers


- [1] Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods
- [2] 

## Fine-tuning method

## Models
| Name     | Paper         |
|----------|--------------|
| LLaMa2 7b    | Developer    | 
| LLaMa2 13b      | Designer     |
| LLaMA-7B    | [1]    | 
| Vicuna-7b-v1.3    | [1]    | 
| Vicuna-13b-v1.3    | [1]    | 

## Datasets 
### Calibration Dataset
| Name     | Paper         |
|----------|--------------|
| BookCorpus   | [1]    | 

### For Finetuning
| Name     | Paper         |
|----------|--------------|
| Alpaca    |   [1]   |
| SlimPajama      |   [1]   |

### For Evaluation
| Name     | Paper         |
|----------|--------------|
| BoolQ   |  [1]   | 
| PIQA    |    [1]  |
| WikiText2   |   [1]   |
| PTB  |   [1]   |
| HellaSwag |    [1]  |
| WinoGrande |   [1]   |
| ARC-easy |   [1]   |
| ARC-challenge |   [1]   |
| OpenbookQA |   [1]   |

## Tools
[lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)







